{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing last names with character-level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "`https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_train.csv.gz`\n",
    "\n",
    "`https://github.com/hunkim/PyTorchZeroToAll/blob/master/data/names_test.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/glove.6B.300d.txt'),\n",
       " PosixPath('data/glove.6B.100d.txt'),\n",
       " PosixPath('data/names_train.csv'),\n",
       " PosixPath('data/names_test.csv'),\n",
       " PosixPath('data/glove.6B.50d.txt'),\n",
       " PosixPath('data/plot.tok.gt9.5000'),\n",
       " PosixPath('data/subjdata.README.1.0'),\n",
       " PosixPath('data/quote.tok.gt9.5000'),\n",
       " PosixPath('data/hour.csv'),\n",
       " PosixPath('data/glove.6B.200d.txt'),\n",
       " PosixPath('data/Readme.txt'),\n",
       " PosixPath('data/train.csv'),\n",
       " PosixPath('data/day.csv'),\n",
       " PosixPath('data/glove.6B.zip'),\n",
       " PosixPath('data/train.csv.zip')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Adsit\",\"Czech\"\r",
      "\r\n",
      "\"Ajdrna\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitsch\",\"Czech\"\r",
      "\r\n",
      "\"Antonowitz\",\"Czech\"\r",
      "\r\n",
      "\"Ballalatak\",\"Czech\"\r",
      "\r\n",
      "\"Ballaltick\",\"Czech\"\r",
      "\r\n",
      "\"Bastl\",\"Czech\"\r",
      "\r\n",
      "\"Baroch\",\"Czech\"\r",
      "\r\n",
      "\"Betlach\",\"Czech\"\r",
      "\r\n",
      "\"Biganska\",\"Czech\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head data/names_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a vocabulary of characters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "vocab2id[\" \"] # I am going to use 0 to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(x, seq_len=15, vocab2id=vocab2id):\n",
    "    x = list(x)\n",
    "    x = np.array([vocab2id[k] for k in x])\n",
    "    z = np.zeros(seq_len, dtype=np.int32)\n",
    "    n = min(seq_len, x.shape[0])\n",
    "    z[seq_len - n:] = x[0:n]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 29, 29, 30, 30, 30],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pad_seq(\"aabbb\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def seq2matrix(x, vocab_len=55):\n",
    "    z = np.zeros((x.shape[0], vocab_len))\n",
    "    z[np.arange(len(x)), x] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        x = seq2matrix(x, self.vocab_len)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = NameDataset(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "val = NameDataset(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n=len(val)\n",
    "train_dl = DataLoader(train, batch_size=batch_size)\n",
    "val_dl = DataLoader(val, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13374, 13374)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 55) 2\n"
     ]
    }
   ],
   "source": [
    "x,y = train[0]\n",
    "print(x.shape,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with one-hot encoding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 15, 55]), torch.Size([2000]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x.shape[0]\n",
    "h = model.initHidden(batch)\n",
    "x = x.float()\n",
    "y = y.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 155])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x[:,0], h), 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ei in range(x.shape[1]):\n",
    "    y_t, h = model(x[:,ei], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.898435354232788"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that just the last x_t is used in the loss\n",
    "# update\n",
    "loss = F.cross_entropy(y_t, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 100\n",
    "n_classes = 18\n",
    "model = CharRNN(vocab_size, hidden_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.00001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        batch = x.shape[0]\n",
    "        h = model.initHidden(batch)\n",
    "        loss = 0\n",
    "        x = x.float()\n",
    "        y = y.long()\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            out, h = model(x[:,t], h)\n",
    "        \n",
    "        loss = F.cross_entropy(out, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metric(model, val_dl):\n",
    "    model.eval()\n",
    "    x, y = next(iter(val_dl))\n",
    "    x = x.float()\n",
    "    y = y.long()\n",
    "    N = x.shape[0]\n",
    "    h = model.initHidden(N)\n",
    "    for t in range(x.shape[1]):\n",
    "        out, h = model(x[:,t], h)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    _, pred = torch.max(out, 1)\n",
    "    acc = pred.eq(y).sum().float()/N\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "hidden_size = 80\n",
    "n_classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, lr, train_dl, val_dl, epochs=20):\n",
    "    optim = get_optimizer(model, lr =lr, wd = 0.0)\n",
    "    for i in range(epochs):\n",
    "        loss = train(model, optim, train_dl)\n",
    "        val_loss, val_acc = val_metric(model, val_dl)\n",
    "        if i%5 == 1: print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (loss, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(vocab_size, hidden_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.117 val loss 1.857 and val accuracy 0.469\n",
      "train loss 1.871 val loss 1.740 and val accuracy 0.469\n",
      "train loss 1.617 val loss 1.502 and val accuracy 0.521\n",
      "train loss 1.407 val loss 1.305 and val accuracy 0.593\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl, val_dl, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.416 val loss 1.245 and val accuracy 0.608\n",
      "train loss 1.210 val loss 1.193 and val accuracy 0.648\n",
      "train loss 1.174 val loss 1.162 and val accuracy 0.656\n",
      "train loss 1.149 val loss 1.137 and val accuracy 0.662\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.001, train_dl, val_dl, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.197 val loss 1.142 and val accuracy 0.651\n",
      "train loss 1.116 val loss 1.107 and val accuracy 0.670\n",
      "train loss 1.093 val loss 1.081 and val accuracy 0.680\n",
      "train loss 1.074 val loss 1.059 and val accuracy 0.686\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.001, train_dl, val_dl, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with character embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDatasetEmb(Dataset):\n",
    "    def __init__(self, path, vocab2id, label2id, seq_len=15, vocab_len=55):\n",
    "        self.df = pd.read_csv(path, header=None)\n",
    "        self.label2id = label2id\n",
    "        self.vocab2id = vocab2id\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_len = vocab_len \n",
    "        self.x = df[0].values\n",
    "        self.y = [self.label2id[l] for l in df[1].values]\n",
    "        self.vocab2id = vocab2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = pad_seq(self.x[idx], self.seq_len, self.vocab2id)\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = NameDatasetEmb(PATH/\"names_train.csv\", vocab2id, label2id)\n",
    "val_2 = NameDatasetEmb(PATH/\"names_test.csv\", vocab2id, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "n = len(val_2)\n",
    "train_dl_2 = DataLoader(train_2, batch_size=batch_size)\n",
    "val_dl_2 = DataLoader(val_2, batch_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3, 32, 47, 37, 48],\n",
       "       dtype=int32),\n",
       " 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size):\n",
    "        super(CharEmbRNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_i2h = nn.Linear(emb_size + hidden_size, hidden_size)\n",
    "        self.linear_h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.emb(x.long())\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = torch.tanh(self.linear_i2h(combined))\n",
    "        output = self.linear_h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, bash_size):\n",
    "        return torch.zeros(bash_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "emb_size = 30\n",
    "hidden_size = 80\n",
    "n_classes = 18\n",
    "model = CharEmbRNN(vocab_size, emb_size, hidden_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.655 val loss 2.008 and val accuracy 0.437\n",
      "train loss 2.554 val loss 1.768 and val accuracy 0.471\n",
      "train loss 1.408 val loss 1.336 and val accuracy 0.580\n",
      "train loss 1.302 val loss 1.210 and val accuracy 0.621\n",
      "train loss 1.208 val loss 1.118 and val accuracy 0.646\n",
      "train loss 1.134 val loss 1.049 and val accuracy 0.670\n",
      "train loss 1.076 val loss 0.994 and val accuracy 0.686\n",
      "train loss 1.027 val loss 0.949 and val accuracy 0.704\n",
      "train loss 0.985 val loss 0.910 and val accuracy 0.719\n",
      "train loss 0.950 val loss 0.879 and val accuracy 0.728\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.075 val loss 1.098 and val accuracy 0.664\n",
      "train loss 0.931 val loss 0.872 and val accuracy 0.732\n",
      "train loss 0.885 val loss 0.814 and val accuracy 0.752\n",
      "train loss 0.865 val loss 0.832 and val accuracy 0.738\n",
      "train loss 0.808 val loss 0.759 and val accuracy 0.769\n",
      "train loss 0.771 val loss 0.721 and val accuracy 0.784\n",
      "train loss 0.752 val loss 0.692 and val accuracy 0.789\n",
      "train loss 0.741 val loss 0.724 and val accuracy 0.778\n",
      "train loss 0.739 val loss 0.712 and val accuracy 0.782\n",
      "train loss 0.684 val loss 0.636 and val accuracy 0.809\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.858 val loss 0.805 and val accuracy 0.759\n",
      "train loss 0.668 val loss 0.624 and val accuracy 0.809\n",
      "train loss 0.633 val loss 0.589 and val accuracy 0.821\n",
      "train loss 0.611 val loss 0.564 and val accuracy 0.827\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.681 val loss 0.746 and val accuracy 0.767\n",
      "train loss 0.592 val loss 0.556 and val accuracy 0.828\n",
      "train loss 0.566 val loss 0.523 and val accuracy 0.838\n",
      "train loss 0.554 val loss 0.508 and val accuracy 0.842\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.01, train_dl_2, val_dl_2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.498 val loss 0.491 and val accuracy 0.851\n",
      "train loss 0.490 val loss 0.486 and val accuracy 0.852\n",
      "train loss 0.487 val loss 0.483 and val accuracy 0.854\n",
      "train loss 0.484 val loss 0.480 and val accuracy 0.855\n",
      "train loss 0.481 val loss 0.476 and val accuracy 0.855\n",
      "train loss 0.478 val loss 0.473 and val accuracy 0.856\n",
      "train loss 0.475 val loss 0.470 and val accuracy 0.857\n",
      "train loss 0.472 val loss 0.467 and val accuracy 0.857\n",
      "train loss 0.469 val loss 0.464 and val accuracy 0.858\n",
      "train loss 0.466 val loss 0.461 and val accuracy 0.859\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, 0.001, train_dl_2, val_dl_2, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "This notebook is a modified version of this tutorial\n",
    "http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html. Here I implement vanilla RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
