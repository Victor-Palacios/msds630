{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying surnames\n",
    "with muti-class logistic regression and bag of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/names_train.csv'), PosixPath('data/names_test.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "Here we split every last name into letters and assign every letter an id. We represent a last name by a vector of letter frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(PATH/\"names_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonowitsch</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonowitz</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ballalatak</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1\n",
       "0         Adsit  Czech\n",
       "1        Ajdrna  Czech\n",
       "2  Antonowitsch  Czech\n",
       "3    Antonowitz  Czech\n",
       "4    Ballalatak  Czech"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab is a list of unique letters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab2id is a dictionary mapping letters to a unique number\n",
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "#vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label2id is a dictionary mapping classes to ids\n",
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_letters = len(vocab)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, vocab2id, label2id, num_letters):\n",
    "    \"\"\" Returns encoded data\n",
    "    \n",
    "    outputs:\n",
    "    data: a np array of shape (df.shape[0], num_letters)\n",
    "          data[i, j] counts the number of times letter vocab[j]\n",
    "          is on observation j\n",
    "    y: np array of len df.shape[0]. Id of the labels of each observation.\n",
    "    \"\"\"\n",
    "    data = np.zeros((df.shape[0], num_letters))\n",
    "    y = np.zeros(df.shape[0])\n",
    "    for i, row in df.iterrows():\n",
    "        y[i] = label2id[row[1]]\n",
    "        for c in list(row[0]):\n",
    "            data[i][vocab2id[c]] +=1\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 55), (6700, 55))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = encode_data(df, vocab2id, label2id, num_letters)\n",
    "x_valid, y_valid = encode_data(val, vocab2id, label2id, num_letters)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'd', 'i', 's', 't']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "[vocab[i] for i, v in enumerate(x_train[0]) if v==1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'a', 'd', 'j', 'n', 'r']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i, v in enumerate(x_train[1]) if v==1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We are going to write a multiclass logistic regression model. Here are the equations:\n",
    "\n",
    "\\begin{align}\n",
    "z_1 & = a_{11}x_1 + \\dots a_{1D}x_D + b_1\\\\\n",
    "z_2 & = a_{21}x_1 + \\dots a_{2D}x_D + b_2 \\\\\n",
    "& \\dots \\\\\n",
    "z_K & = a_{K1}x_1 + \\dots a_{KD}x_D + b_K\n",
    "\\end{align}\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{ \\sum_{i=1}^K e^{z_i}}$$\n",
    "\n",
    "\n",
    "Here the observations are $D$ dimensional vectors $x = (x_1, \\dots, x_D)$.\n",
    "\n",
    "In order to get multiclass logistic regression, we do a linear transformation and then a softmax transformation.\n",
    "\n",
    "For numerical reasons, it is better not to apply the softmax directly after the linear transformation but to apply it together with the loss function. The loss function `F.cross_entropy` combines log_softmax and nll_loss in a single function. Therefore to write the model just do the linear transformation with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiLogisticRegression, self).__init__()\n",
    "        ## CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, x_train, y_train, x_valid, y_valid, epochs, lr=0.01, wd=1e-4):\n",
    "    ## get an optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ## convert your training data to pytorch tensors\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## evaluate your training data to get y_hat\n",
    "           \n",
    "        ## compute your loss\n",
    "        \n",
    "        ## zero_grad\n",
    "        \n",
    "        ## compute gradients\n",
    "        \n",
    "        ## call gradient descent\n",
    "        \n",
    "        ## call valid_metrics(model, x_valid, y_valid)\n",
    "        ## print train loss, valid loss and potentially valid accuracy\n",
    "        val_loss, val_acc = valid_metrics(model, x_valid, y_valid)\n",
    "        if i%10 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "                  (loss.item(), val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, x_valid, y_valid):\n",
    "    model.eval()\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "   \n",
    "\n",
    "    return loss.item(), val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLogisticRegression( , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.144 val loss 1.733 and val accuracy 0.491\n",
      "train loss 1.319 val loss 1.286 and val accuracy 0.633\n",
      "train loss 1.130 val loss 1.112 and val accuracy 0.656\n",
      "train loss 1.045 val loss 1.038 and val accuracy 0.672\n",
      "train loss 1.007 val loss 1.004 and val accuracy 0.680\n",
      "train loss 0.986 val loss 0.984 and val accuracy 0.686\n",
      "train loss 0.974 val loss 0.973 and val accuracy 0.684\n",
      "train loss 0.967 val loss 0.966 and val accuracy 0.686\n",
      "train loss 0.961 val loss 0.961 and val accuracy 0.689\n",
      "train loss 0.957 val loss 0.956 and val accuracy 0.689\n",
      "train loss 0.954 val loss 0.953 and val accuracy 0.690\n",
      "train loss 0.951 val loss 0.951 and val accuracy 0.691\n",
      "train loss 0.949 val loss 0.948 and val accuracy 0.691\n",
      "train loss 0.947 val loss 0.947 and val accuracy 0.691\n",
      "train loss 0.945 val loss 0.945 and val accuracy 0.691\n",
      "train loss 0.944 val loss 0.944 and val accuracy 0.691\n",
      "train loss 0.943 val loss 0.943 and val accuracy 0.692\n",
      "train loss 0.942 val loss 0.942 and val accuracy 0.692\n",
      "train loss 0.941 val loss 0.941 and val accuracy 0.692\n",
      "train loss 0.940 val loss 0.940 and val accuracy 0.693\n",
      "train loss 0.940 val loss 0.939 and val accuracy 0.692\n",
      "train loss 0.939 val loss 0.939 and val accuracy 0.692\n",
      "train loss 0.938 val loss 0.938 and val accuracy 0.692\n",
      "train loss 0.938 val loss 0.938 and val accuracy 0.692\n",
      "train loss 0.937 val loss 0.937 and val accuracy 0.693\n",
      "train loss 0.937 val loss 0.937 and val accuracy 0.693\n",
      "train loss 0.937 val loss 0.937 and val accuracy 0.693\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.693\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.693\n",
      "train loss 0.936 val loss 0.936 and val accuracy 0.693\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, x_train, y_train, x_valid, y_valid, 300, lr=0.1, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
