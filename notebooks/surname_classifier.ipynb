{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying surnames\n",
    "with muti-class logistic regression and bag of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_train.csv.gz \n",
    "    ! wget https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/names_test.csv.gz \n",
    "    ! mkdir -p data\n",
    "    ! gunzip names_train.csv.gz \n",
    "    ! gunzip names_test.csv.gz\n",
    "    ! mv names*.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/names_train.csv'), PosixPath('data/names_test.csv')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "Here we split every last name into letters and assign every letter an id. We represent a last name by a vector of letter frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"names_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(PATH/\"names_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adsit</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajdrna</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonowitsch</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Antonowitz</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ballalatak</td>\n",
       "      <td>Czech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1\n",
       "0         Adsit  Czech\n",
       "1        Ajdrna  Czech\n",
       "2  Antonowitsch  Czech\n",
       "3    Antonowitz  Czech\n",
       "4    Ballalatak  Czech"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', \"'\", ',', 'A', 'B']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vocab is a list of unique letters\n",
    "letters = [list(l) for l in df[0].values]\n",
    "vocab = sorted(list(set(np.concatenate(np.array(letters)))))\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab2id is a dictionary mapping letters to a unique number\n",
    "vocab2id = {key:i for i, key in enumerate(vocab)}\n",
    "#vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label2id is a dictionary mapping classes to ids\n",
    "labels = sorted(df[1].unique())\n",
    "label2id = {key:i for i, key in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_letters = len(vocab)\n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, vocab2id, label2id, num_letters):\n",
    "    \"\"\" Returns encoded data\n",
    "    \n",
    "    outputs:\n",
    "    data: a np array of shape (df.shape[0], num_letters)\n",
    "          data[i, j] counts the number of times letter vocab[j]\n",
    "          is on observation j\n",
    "    y: np array of len df.shape[0]. Id of the labels of each observation.\n",
    "    \"\"\"\n",
    "    data = np.zeros((df.shape[0], num_letters))\n",
    "    y = np.zeros(df.shape[0])\n",
    "    for i, row in df.iterrows():\n",
    "        y[i] = label2id[row[1]]\n",
    "        for c in list(row[0]):\n",
    "            data[i][vocab2id[c]] +=1\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13374, 55), (6700, 55))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = encode_data(df, vocab2id, label2id, num_letters)\n",
    "x_valid, y_valid = encode_data(val, vocab2id, label2id, num_letters)\n",
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'd', 'i', 's', 't']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking\n",
    "[vocab[i] for i, v in enumerate(train[0]) if v==1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'a', 'd', 'j', 'n', 'r']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i, v in enumerate(train[1]) if v==1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We are going to write a multiclass logistic regression model. Here are the equations:\n",
    "\n",
    "\\begin{align}\n",
    "z_1 & = a_{11}x_1 + \\dots a_{1D}x_D + b_1\\\\\n",
    "z_2 & = a_{21}x_1 + \\dots a_{2D}x_D + b_2 \\\\\n",
    "& \\dots \\\\\n",
    "z_K & = a_{K1}x_1 + \\dots a_{KD}x_D + b_K\n",
    "\\end{align}\n",
    "\n",
    "$$\\hat{y}_k = \\frac{e^{z_k}}{ \\sum_{i=1}^K e^{z_i}}$$\n",
    "\n",
    "\n",
    "Here the observations are $D$ dimensional vectors $x = (x_1, \\dots, x_D)$.\n",
    "\n",
    "In order to get multiclass logistic regression, we do a linear transformation and then a softmax transformation.\n",
    "\n",
    "For numerical reasons, it is better not to apply the softmax directly after the linear transformation but to apply it together with the loss function. The loss function `F.cross_entropy` combines log_softmax and nll_loss in a single function. Therefore to write the model just do the linear transformation with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(55, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([18, 55]), torch.Size([18])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in linear.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train[:5]\n",
    "x = torch.FloatTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 18])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLogisticRegression(55, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([18, 55]), torch.Size([18])]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y_train[:5]\n",
    "y = torch.LongTensor(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1813,  0.0577, -0.1362,  0.0857, -0.0179, -0.0444, -0.1344, -0.0322,\n",
       "          0.0118, -0.1159, -0.0112,  0.1755,  0.1616,  0.0831,  0.1713,  0.1547,\n",
       "         -0.2563, -0.0202],\n",
       "        [ 0.1791,  0.1832, -0.2689, -0.0783, -0.0911, -0.2611, -0.1542,  0.2432,\n",
       "          0.2308, -0.0384, -0.1001, -0.1897,  0.1539,  0.2216, -0.1053,  0.4789,\n",
       "         -0.2517,  0.0345],\n",
       "        [ 0.1286, -0.3850,  0.1606,  0.0854,  0.0234,  0.1580, -0.1072, -0.5261,\n",
       "         -0.0843, -0.0926, -0.5665,  0.2665, -0.0182, -0.1218, -0.4369,  0.1597,\n",
       "         -0.1874,  0.0051],\n",
       "        [ 0.4305, -0.4202,  0.2180, -0.1700,  0.1111,  0.3632, -0.3402, -0.1704,\n",
       "         -0.2563, -0.1188, -0.3392,  0.3766,  0.0647, -0.0113, -0.3780,  0.2348,\n",
       "         -0.1337,  0.1811],\n",
       "        [ 0.3074, -0.2640, -0.1997,  0.1764, -0.7316, -0.3513,  0.3223, -0.4395,\n",
       "          0.7176, -0.3365,  0.1911, -0.2637,  0.2245, -0.1673, -0.0556,  0.6120,\n",
       "         -0.3820, -0.0515]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(y_hat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 15, 11,  0,  8])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1395, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, x_train, y_train, x_valid, y_valid, epochs, lr=0.01, wd=1e-4):\n",
    "    ## get an optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ## convert your training data to pytorch tensors\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## evaluate your training data to get y_hat\n",
    "        y_hat = model(x)    \n",
    "        ## compute your loss\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        ## zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        ## compute gradients\n",
    "        loss.backward()\n",
    "        ## call gradient descent\n",
    "        optimizer.step()\n",
    "        ## call valid_metrics(model, x_valid, y_valid)\n",
    "        ## print train loss, valid loss and potentially valid accuracy\n",
    "        val_loss, val_acc = valid_metrics(model, x_valid, y_valid)\n",
    "        if True:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "                  (loss.item(), val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics(model, x_valid, y_valid):\n",
    "    model.eval()\n",
    "    x = torch.FloatTensor(x_valid)\n",
    "    y = torch.LongTensor(y_valid)\n",
    "    y_out = model(x)  \n",
    "    loss = F.cross_entropy(y_out, y)\n",
    "    _, y_hat = torch.max(y_out, 1)\n",
    "    val_acc = y_hat.eq(y).sum().float()/y.size(0)\n",
    "    return loss.item(), val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLogisticRegression(55, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.995 val loss 2.984 and val accuracy 0.011\n",
      "train loss 2.984 val loss 2.974 and val accuracy 0.013\n",
      "train loss 2.974 val loss 2.964 and val accuracy 0.016\n",
      "train loss 2.963 val loss 2.953 and val accuracy 0.019\n",
      "train loss 2.953 val loss 2.943 and val accuracy 0.020\n",
      "train loss 2.942 val loss 2.932 and val accuracy 0.022\n",
      "train loss 2.932 val loss 2.922 and val accuracy 0.026\n",
      "train loss 2.922 val loss 2.912 and val accuracy 0.028\n",
      "train loss 2.911 val loss 2.902 and val accuracy 0.031\n",
      "train loss 2.901 val loss 2.892 and val accuracy 0.034\n",
      "train loss 2.891 val loss 2.881 and val accuracy 0.036\n",
      "train loss 2.881 val loss 2.871 and val accuracy 0.043\n",
      "train loss 2.870 val loss 2.861 and val accuracy 0.048\n",
      "train loss 2.860 val loss 2.851 and val accuracy 0.053\n",
      "train loss 2.850 val loss 2.841 and val accuracy 0.058\n",
      "train loss 2.840 val loss 2.831 and val accuracy 0.064\n",
      "train loss 2.830 val loss 2.821 and val accuracy 0.071\n",
      "train loss 2.820 val loss 2.811 and val accuracy 0.080\n",
      "train loss 2.810 val loss 2.802 and val accuracy 0.089\n",
      "train loss 2.800 val loss 2.792 and val accuracy 0.099\n",
      "train loss 2.790 val loss 2.782 and val accuracy 0.110\n",
      "train loss 2.780 val loss 2.772 and val accuracy 0.120\n",
      "train loss 2.771 val loss 2.763 and val accuracy 0.132\n",
      "train loss 2.761 val loss 2.753 and val accuracy 0.144\n",
      "train loss 2.751 val loss 2.743 and val accuracy 0.157\n",
      "train loss 2.741 val loss 2.734 and val accuracy 0.170\n",
      "train loss 2.732 val loss 2.724 and val accuracy 0.182\n",
      "train loss 2.722 val loss 2.715 and val accuracy 0.193\n",
      "train loss 2.713 val loss 2.705 and val accuracy 0.207\n",
      "train loss 2.703 val loss 2.696 and val accuracy 0.220\n",
      "train loss 2.694 val loss 2.686 and val accuracy 0.235\n",
      "train loss 2.684 val loss 2.677 and val accuracy 0.249\n",
      "train loss 2.675 val loss 2.668 and val accuracy 0.268\n",
      "train loss 2.666 val loss 2.659 and val accuracy 0.278\n",
      "train loss 2.656 val loss 2.649 and val accuracy 0.294\n",
      "train loss 2.647 val loss 2.640 and val accuracy 0.307\n",
      "train loss 2.638 val loss 2.631 and val accuracy 0.322\n",
      "train loss 2.629 val loss 2.622 and val accuracy 0.335\n",
      "train loss 2.620 val loss 2.613 and val accuracy 0.347\n",
      "train loss 2.610 val loss 2.604 and val accuracy 0.357\n",
      "train loss 2.601 val loss 2.595 and val accuracy 0.370\n",
      "train loss 2.592 val loss 2.587 and val accuracy 0.379\n",
      "train loss 2.584 val loss 2.578 and val accuracy 0.391\n",
      "train loss 2.575 val loss 2.569 and val accuracy 0.405\n",
      "train loss 2.566 val loss 2.560 and val accuracy 0.413\n",
      "train loss 2.557 val loss 2.552 and val accuracy 0.425\n",
      "train loss 2.548 val loss 2.543 and val accuracy 0.432\n",
      "train loss 2.540 val loss 2.534 and val accuracy 0.440\n",
      "train loss 2.531 val loss 2.526 and val accuracy 0.446\n",
      "train loss 2.523 val loss 2.517 and val accuracy 0.453\n",
      "train loss 2.514 val loss 2.509 and val accuracy 0.460\n",
      "train loss 2.506 val loss 2.501 and val accuracy 0.466\n",
      "train loss 2.497 val loss 2.492 and val accuracy 0.471\n",
      "train loss 2.489 val loss 2.484 and val accuracy 0.476\n",
      "train loss 2.480 val loss 2.476 and val accuracy 0.480\n",
      "train loss 2.472 val loss 2.468 and val accuracy 0.483\n",
      "train loss 2.464 val loss 2.459 and val accuracy 0.487\n",
      "train loss 2.456 val loss 2.451 and val accuracy 0.489\n",
      "train loss 2.448 val loss 2.443 and val accuracy 0.490\n",
      "train loss 2.439 val loss 2.435 and val accuracy 0.490\n",
      "train loss 2.431 val loss 2.427 and val accuracy 0.492\n",
      "train loss 2.423 val loss 2.420 and val accuracy 0.494\n",
      "train loss 2.416 val loss 2.412 and val accuracy 0.497\n",
      "train loss 2.408 val loss 2.404 and val accuracy 0.500\n",
      "train loss 2.400 val loss 2.396 and val accuracy 0.501\n",
      "train loss 2.392 val loss 2.388 and val accuracy 0.501\n",
      "train loss 2.384 val loss 2.381 and val accuracy 0.503\n",
      "train loss 2.377 val loss 2.373 and val accuracy 0.504\n",
      "train loss 2.369 val loss 2.366 and val accuracy 0.504\n",
      "train loss 2.361 val loss 2.358 and val accuracy 0.505\n",
      "train loss 2.354 val loss 2.351 and val accuracy 0.507\n",
      "train loss 2.346 val loss 2.343 and val accuracy 0.508\n",
      "train loss 2.339 val loss 2.336 and val accuracy 0.510\n",
      "train loss 2.332 val loss 2.329 and val accuracy 0.511\n",
      "train loss 2.324 val loss 2.322 and val accuracy 0.511\n",
      "train loss 2.317 val loss 2.314 and val accuracy 0.511\n",
      "train loss 2.310 val loss 2.307 and val accuracy 0.512\n",
      "train loss 2.303 val loss 2.300 and val accuracy 0.513\n",
      "train loss 2.295 val loss 2.293 and val accuracy 0.512\n",
      "train loss 2.288 val loss 2.286 and val accuracy 0.514\n",
      "train loss 2.281 val loss 2.279 and val accuracy 0.516\n",
      "train loss 2.274 val loss 2.272 and val accuracy 0.518\n",
      "train loss 2.268 val loss 2.265 and val accuracy 0.519\n",
      "train loss 2.261 val loss 2.259 and val accuracy 0.519\n",
      "train loss 2.254 val loss 2.252 and val accuracy 0.520\n",
      "train loss 2.247 val loss 2.245 and val accuracy 0.521\n",
      "train loss 2.240 val loss 2.239 and val accuracy 0.520\n",
      "train loss 2.234 val loss 2.232 and val accuracy 0.521\n",
      "train loss 2.227 val loss 2.225 and val accuracy 0.522\n",
      "train loss 2.220 val loss 2.219 and val accuracy 0.522\n",
      "train loss 2.214 val loss 2.213 and val accuracy 0.523\n",
      "train loss 2.207 val loss 2.206 and val accuracy 0.523\n",
      "train loss 2.201 val loss 2.200 and val accuracy 0.523\n",
      "train loss 2.195 val loss 2.193 and val accuracy 0.523\n",
      "train loss 2.188 val loss 2.187 and val accuracy 0.523\n",
      "train loss 2.182 val loss 2.181 and val accuracy 0.524\n",
      "train loss 2.176 val loss 2.175 and val accuracy 0.523\n",
      "train loss 2.170 val loss 2.169 and val accuracy 0.524\n",
      "train loss 2.163 val loss 2.163 and val accuracy 0.524\n",
      "train loss 2.157 val loss 2.157 and val accuracy 0.525\n",
      "train loss 2.151 val loss 2.151 and val accuracy 0.525\n",
      "train loss 2.145 val loss 2.145 and val accuracy 0.525\n",
      "train loss 2.139 val loss 2.139 and val accuracy 0.525\n",
      "train loss 2.133 val loss 2.133 and val accuracy 0.525\n",
      "train loss 2.128 val loss 2.127 and val accuracy 0.525\n",
      "train loss 2.122 val loss 2.121 and val accuracy 0.525\n",
      "train loss 2.116 val loss 2.116 and val accuracy 0.525\n",
      "train loss 2.110 val loss 2.110 and val accuracy 0.525\n",
      "train loss 2.105 val loss 2.104 and val accuracy 0.525\n",
      "train loss 2.099 val loss 2.099 and val accuracy 0.524\n",
      "train loss 2.093 val loss 2.093 and val accuracy 0.525\n",
      "train loss 2.088 val loss 2.088 and val accuracy 0.525\n",
      "train loss 2.082 val loss 2.082 and val accuracy 0.525\n",
      "train loss 2.077 val loss 2.077 and val accuracy 0.525\n",
      "train loss 2.071 val loss 2.072 and val accuracy 0.526\n",
      "train loss 2.066 val loss 2.066 and val accuracy 0.526\n",
      "train loss 2.061 val loss 2.061 and val accuracy 0.526\n",
      "train loss 2.056 val loss 2.056 and val accuracy 0.526\n",
      "train loss 2.050 val loss 2.051 and val accuracy 0.527\n",
      "train loss 2.045 val loss 2.046 and val accuracy 0.526\n",
      "train loss 2.040 val loss 2.041 and val accuracy 0.526\n",
      "train loss 2.035 val loss 2.036 and val accuracy 0.526\n",
      "train loss 2.030 val loss 2.031 and val accuracy 0.526\n",
      "train loss 2.025 val loss 2.026 and val accuracy 0.526\n",
      "train loss 2.020 val loss 2.021 and val accuracy 0.526\n",
      "train loss 2.015 val loss 2.016 and val accuracy 0.526\n",
      "train loss 2.010 val loss 2.011 and val accuracy 0.526\n",
      "train loss 2.005 val loss 2.006 and val accuracy 0.526\n",
      "train loss 2.000 val loss 2.001 and val accuracy 0.526\n",
      "train loss 1.996 val loss 1.997 and val accuracy 0.526\n",
      "train loss 1.991 val loss 1.992 and val accuracy 0.527\n",
      "train loss 1.986 val loss 1.987 and val accuracy 0.527\n",
      "train loss 1.982 val loss 1.983 and val accuracy 0.527\n",
      "train loss 1.977 val loss 1.978 and val accuracy 0.527\n",
      "train loss 1.972 val loss 1.974 and val accuracy 0.527\n",
      "train loss 1.968 val loss 1.969 and val accuracy 0.527\n",
      "train loss 1.963 val loss 1.965 and val accuracy 0.527\n",
      "train loss 1.959 val loss 1.961 and val accuracy 0.527\n",
      "train loss 1.955 val loss 1.956 and val accuracy 0.527\n",
      "train loss 1.950 val loss 1.952 and val accuracy 0.526\n",
      "train loss 1.946 val loss 1.948 and val accuracy 0.526\n",
      "train loss 1.942 val loss 1.943 and val accuracy 0.526\n",
      "train loss 1.937 val loss 1.939 and val accuracy 0.525\n",
      "train loss 1.933 val loss 1.935 and val accuracy 0.526\n",
      "train loss 1.929 val loss 1.931 and val accuracy 0.526\n",
      "train loss 1.925 val loss 1.927 and val accuracy 0.526\n",
      "train loss 1.921 val loss 1.923 and val accuracy 0.526\n",
      "train loss 1.917 val loss 1.919 and val accuracy 0.526\n",
      "train loss 1.913 val loss 1.915 and val accuracy 0.526\n",
      "train loss 1.909 val loss 1.911 and val accuracy 0.526\n",
      "train loss 1.905 val loss 1.907 and val accuracy 0.525\n",
      "train loss 1.901 val loss 1.903 and val accuracy 0.525\n",
      "train loss 1.897 val loss 1.899 and val accuracy 0.521\n",
      "train loss 1.893 val loss 1.895 and val accuracy 0.521\n",
      "train loss 1.889 val loss 1.891 and val accuracy 0.521\n",
      "train loss 1.885 val loss 1.888 and val accuracy 0.521\n",
      "train loss 1.882 val loss 1.884 and val accuracy 0.521\n",
      "train loss 1.878 val loss 1.880 and val accuracy 0.521\n",
      "train loss 1.874 val loss 1.877 and val accuracy 0.520\n",
      "train loss 1.871 val loss 1.873 and val accuracy 0.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.867 val loss 1.869 and val accuracy 0.519\n",
      "train loss 1.863 val loss 1.866 and val accuracy 0.520\n",
      "train loss 1.860 val loss 1.862 and val accuracy 0.520\n",
      "train loss 1.856 val loss 1.859 and val accuracy 0.520\n",
      "train loss 1.853 val loss 1.855 and val accuracy 0.520\n",
      "train loss 1.849 val loss 1.852 and val accuracy 0.520\n",
      "train loss 1.846 val loss 1.849 and val accuracy 0.520\n",
      "train loss 1.843 val loss 1.845 and val accuracy 0.520\n",
      "train loss 1.839 val loss 1.842 and val accuracy 0.519\n",
      "train loss 1.836 val loss 1.839 and val accuracy 0.520\n",
      "train loss 1.833 val loss 1.835 and val accuracy 0.520\n",
      "train loss 1.829 val loss 1.832 and val accuracy 0.520\n",
      "train loss 1.826 val loss 1.829 and val accuracy 0.520\n",
      "train loss 1.823 val loss 1.826 and val accuracy 0.519\n",
      "train loss 1.820 val loss 1.822 and val accuracy 0.519\n",
      "train loss 1.816 val loss 1.819 and val accuracy 0.519\n",
      "train loss 1.813 val loss 1.816 and val accuracy 0.519\n",
      "train loss 1.810 val loss 1.813 and val accuracy 0.519\n",
      "train loss 1.807 val loss 1.810 and val accuracy 0.519\n",
      "train loss 1.804 val loss 1.807 and val accuracy 0.519\n",
      "train loss 1.801 val loss 1.804 and val accuracy 0.519\n",
      "train loss 1.798 val loss 1.801 and val accuracy 0.519\n",
      "train loss 1.795 val loss 1.798 and val accuracy 0.520\n",
      "train loss 1.792 val loss 1.795 and val accuracy 0.520\n",
      "train loss 1.789 val loss 1.792 and val accuracy 0.520\n",
      "train loss 1.786 val loss 1.789 and val accuracy 0.520\n",
      "train loss 1.783 val loss 1.786 and val accuracy 0.520\n",
      "train loss 1.781 val loss 1.784 and val accuracy 0.519\n",
      "train loss 1.778 val loss 1.781 and val accuracy 0.519\n",
      "train loss 1.775 val loss 1.778 and val accuracy 0.519\n",
      "train loss 1.772 val loss 1.775 and val accuracy 0.519\n",
      "train loss 1.769 val loss 1.773 and val accuracy 0.519\n",
      "train loss 1.767 val loss 1.770 and val accuracy 0.519\n",
      "train loss 1.764 val loss 1.767 and val accuracy 0.519\n",
      "train loss 1.761 val loss 1.765 and val accuracy 0.519\n",
      "train loss 1.759 val loss 1.762 and val accuracy 0.519\n",
      "train loss 1.756 val loss 1.759 and val accuracy 0.519\n",
      "train loss 1.753 val loss 1.757 and val accuracy 0.519\n",
      "train loss 1.751 val loss 1.754 and val accuracy 0.519\n",
      "train loss 1.748 val loss 1.752 and val accuracy 0.519\n",
      "train loss 1.746 val loss 1.749 and val accuracy 0.519\n",
      "train loss 1.743 val loss 1.747 and val accuracy 0.519\n",
      "train loss 1.741 val loss 1.744 and val accuracy 0.519\n",
      "train loss 1.738 val loss 1.742 and val accuracy 0.519\n",
      "train loss 1.736 val loss 1.739 and val accuracy 0.519\n",
      "train loss 1.733 val loss 1.737 and val accuracy 0.519\n",
      "train loss 1.731 val loss 1.734 and val accuracy 0.519\n",
      "train loss 1.728 val loss 1.732 and val accuracy 0.519\n",
      "train loss 1.726 val loss 1.730 and val accuracy 0.520\n",
      "train loss 1.724 val loss 1.727 and val accuracy 0.520\n",
      "train loss 1.721 val loss 1.725 and val accuracy 0.520\n",
      "train loss 1.719 val loss 1.723 and val accuracy 0.520\n",
      "train loss 1.717 val loss 1.720 and val accuracy 0.520\n",
      "train loss 1.714 val loss 1.718 and val accuracy 0.520\n",
      "train loss 1.712 val loss 1.716 and val accuracy 0.520\n",
      "train loss 1.710 val loss 1.714 and val accuracy 0.520\n",
      "train loss 1.708 val loss 1.711 and val accuracy 0.520\n",
      "train loss 1.705 val loss 1.709 and val accuracy 0.520\n",
      "train loss 1.703 val loss 1.707 and val accuracy 0.520\n",
      "train loss 1.701 val loss 1.705 and val accuracy 0.520\n",
      "train loss 1.699 val loss 1.703 and val accuracy 0.520\n",
      "train loss 1.697 val loss 1.700 and val accuracy 0.520\n",
      "train loss 1.695 val loss 1.698 and val accuracy 0.520\n",
      "train loss 1.692 val loss 1.696 and val accuracy 0.520\n",
      "train loss 1.690 val loss 1.694 and val accuracy 0.520\n",
      "train loss 1.688 val loss 1.692 and val accuracy 0.520\n",
      "train loss 1.686 val loss 1.690 and val accuracy 0.520\n",
      "train loss 1.684 val loss 1.688 and val accuracy 0.520\n",
      "train loss 1.682 val loss 1.686 and val accuracy 0.520\n",
      "train loss 1.680 val loss 1.684 and val accuracy 0.520\n",
      "train loss 1.678 val loss 1.682 and val accuracy 0.520\n",
      "train loss 1.676 val loss 1.680 and val accuracy 0.520\n",
      "train loss 1.674 val loss 1.678 and val accuracy 0.520\n",
      "train loss 1.672 val loss 1.676 and val accuracy 0.520\n",
      "train loss 1.670 val loss 1.674 and val accuracy 0.520\n",
      "train loss 1.668 val loss 1.672 and val accuracy 0.523\n",
      "train loss 1.666 val loss 1.670 and val accuracy 0.525\n",
      "train loss 1.664 val loss 1.668 and val accuracy 0.525\n",
      "train loss 1.663 val loss 1.667 and val accuracy 0.524\n",
      "train loss 1.661 val loss 1.665 and val accuracy 0.524\n",
      "train loss 1.659 val loss 1.663 and val accuracy 0.525\n",
      "train loss 1.657 val loss 1.661 and val accuracy 0.525\n",
      "train loss 1.655 val loss 1.659 and val accuracy 0.525\n",
      "train loss 1.653 val loss 1.657 and val accuracy 0.525\n",
      "train loss 1.651 val loss 1.656 and val accuracy 0.525\n",
      "train loss 1.650 val loss 1.654 and val accuracy 0.525\n",
      "train loss 1.648 val loss 1.652 and val accuracy 0.525\n",
      "train loss 1.646 val loss 1.650 and val accuracy 0.526\n",
      "train loss 1.644 val loss 1.649 and val accuracy 0.526\n",
      "train loss 1.643 val loss 1.647 and val accuracy 0.526\n",
      "train loss 1.641 val loss 1.645 and val accuracy 0.526\n",
      "train loss 1.639 val loss 1.643 and val accuracy 0.526\n",
      "train loss 1.637 val loss 1.642 and val accuracy 0.526\n",
      "train loss 1.636 val loss 1.640 and val accuracy 0.526\n",
      "train loss 1.634 val loss 1.638 and val accuracy 0.526\n",
      "train loss 1.632 val loss 1.637 and val accuracy 0.526\n",
      "train loss 1.631 val loss 1.635 and val accuracy 0.526\n",
      "train loss 1.629 val loss 1.633 and val accuracy 0.526\n",
      "train loss 1.627 val loss 1.632 and val accuracy 0.527\n",
      "train loss 1.626 val loss 1.630 and val accuracy 0.527\n",
      "train loss 1.624 val loss 1.629 and val accuracy 0.527\n",
      "train loss 1.623 val loss 1.627 and val accuracy 0.527\n",
      "train loss 1.621 val loss 1.625 and val accuracy 0.527\n",
      "train loss 1.619 val loss 1.624 and val accuracy 0.527\n",
      "train loss 1.618 val loss 1.622 and val accuracy 0.527\n",
      "train loss 1.616 val loss 1.621 and val accuracy 0.528\n",
      "train loss 1.615 val loss 1.619 and val accuracy 0.528\n",
      "train loss 1.613 val loss 1.618 and val accuracy 0.528\n",
      "train loss 1.612 val loss 1.616 and val accuracy 0.529\n",
      "train loss 1.610 val loss 1.614 and val accuracy 0.529\n",
      "train loss 1.608 val loss 1.613 and val accuracy 0.531\n",
      "train loss 1.607 val loss 1.611 and val accuracy 0.531\n",
      "train loss 1.605 val loss 1.610 and val accuracy 0.532\n",
      "train loss 1.604 val loss 1.608 and val accuracy 0.532\n",
      "train loss 1.602 val loss 1.607 and val accuracy 0.532\n",
      "train loss 1.601 val loss 1.606 and val accuracy 0.532\n",
      "train loss 1.600 val loss 1.604 and val accuracy 0.532\n",
      "train loss 1.598 val loss 1.603 and val accuracy 0.532\n",
      "train loss 1.597 val loss 1.601 and val accuracy 0.533\n",
      "train loss 1.595 val loss 1.600 and val accuracy 0.533\n",
      "train loss 1.594 val loss 1.598 and val accuracy 0.534\n",
      "train loss 1.592 val loss 1.597 and val accuracy 0.534\n",
      "train loss 1.591 val loss 1.596 and val accuracy 0.534\n",
      "train loss 1.589 val loss 1.594 and val accuracy 0.534\n",
      "train loss 1.588 val loss 1.593 and val accuracy 0.534\n",
      "train loss 1.587 val loss 1.591 and val accuracy 0.534\n",
      "train loss 1.585 val loss 1.590 and val accuracy 0.534\n",
      "train loss 1.584 val loss 1.589 and val accuracy 0.535\n",
      "train loss 1.583 val loss 1.587 and val accuracy 0.535\n",
      "train loss 1.581 val loss 1.586 and val accuracy 0.534\n",
      "train loss 1.580 val loss 1.585 and val accuracy 0.534\n",
      "train loss 1.578 val loss 1.583 and val accuracy 0.534\n",
      "train loss 1.577 val loss 1.582 and val accuracy 0.535\n",
      "train loss 1.576 val loss 1.581 and val accuracy 0.535\n",
      "train loss 1.574 val loss 1.579 and val accuracy 0.535\n",
      "train loss 1.573 val loss 1.578 and val accuracy 0.536\n",
      "train loss 1.572 val loss 1.577 and val accuracy 0.536\n",
      "train loss 1.571 val loss 1.575 and val accuracy 0.536\n",
      "train loss 1.569 val loss 1.574 and val accuracy 0.537\n",
      "train loss 1.568 val loss 1.573 and val accuracy 0.537\n"
     ]
    }
   ],
   "source": [
    "train_epochs(model, x_train, y_train, x_valid, y_valid, 300, lr=0.001, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.840 val loss 2.379 and val accuracy 0.479\n",
      "train loss 2.372 val loss 2.023 and val accuracy 0.511\n",
      "train loss 2.015 val loss 1.788 and val accuracy 0.512\n",
      "train loss 1.781 val loss 1.653 and val accuracy 0.517\n",
      "train loss 1.646 val loss 1.582 and val accuracy 0.510\n",
      "train loss 1.578 val loss 1.549 and val accuracy 0.504\n",
      "train loss 1.546 val loss 1.529 and val accuracy 0.503\n",
      "train loss 1.527 val loss 1.511 and val accuracy 0.511\n",
      "train loss 1.510 val loss 1.489 and val accuracy 0.524\n",
      "train loss 1.488 val loss 1.464 and val accuracy 0.540\n",
      "train loss 1.462 val loss 1.438 and val accuracy 0.566\n",
      "train loss 1.436 val loss 1.414 and val accuracy 0.586\n",
      "train loss 1.410 val loss 1.392 and val accuracy 0.601\n",
      "train loss 1.387 val loss 1.372 and val accuracy 0.614\n",
      "train loss 1.366 val loss 1.352 and val accuracy 0.622\n",
      "train loss 1.345 val loss 1.331 and val accuracy 0.623\n",
      "train loss 1.323 val loss 1.309 and val accuracy 0.626\n",
      "train loss 1.300 val loss 1.287 and val accuracy 0.629\n",
      "train loss 1.277 val loss 1.266 and val accuracy 0.629\n",
      "train loss 1.255 val loss 1.247 and val accuracy 0.630\n",
      "train loss 1.235 val loss 1.232 and val accuracy 0.632\n",
      "train loss 1.219 val loss 1.220 and val accuracy 0.633\n",
      "train loss 1.206 val loss 1.211 and val accuracy 0.632\n",
      "train loss 1.196 val loss 1.203 and val accuracy 0.632\n",
      "train loss 1.188 val loss 1.196 and val accuracy 0.634\n",
      "train loss 1.180 val loss 1.188 and val accuracy 0.636\n",
      "train loss 1.172 val loss 1.179 and val accuracy 0.636\n",
      "train loss 1.162 val loss 1.169 and val accuracy 0.641\n",
      "train loss 1.152 val loss 1.160 and val accuracy 0.645\n",
      "train loss 1.142 val loss 1.151 and val accuracy 0.647\n",
      "train loss 1.133 val loss 1.144 and val accuracy 0.649\n",
      "train loss 1.125 val loss 1.138 and val accuracy 0.652\n",
      "train loss 1.118 val loss 1.133 and val accuracy 0.652\n",
      "train loss 1.113 val loss 1.128 and val accuracy 0.652\n",
      "train loss 1.108 val loss 1.124 and val accuracy 0.653\n",
      "train loss 1.103 val loss 1.120 and val accuracy 0.655\n",
      "train loss 1.099 val loss 1.116 and val accuracy 0.657\n",
      "train loss 1.094 val loss 1.111 and val accuracy 0.659\n",
      "train loss 1.089 val loss 1.106 and val accuracy 0.659\n",
      "train loss 1.084 val loss 1.102 and val accuracy 0.659\n",
      "train loss 1.079 val loss 1.097 and val accuracy 0.659\n",
      "train loss 1.074 val loss 1.093 and val accuracy 0.659\n",
      "train loss 1.070 val loss 1.090 and val accuracy 0.659\n",
      "train loss 1.066 val loss 1.086 and val accuracy 0.660\n",
      "train loss 1.062 val loss 1.083 and val accuracy 0.662\n",
      "train loss 1.059 val loss 1.081 and val accuracy 0.662\n",
      "train loss 1.056 val loss 1.078 and val accuracy 0.665\n",
      "train loss 1.053 val loss 1.076 and val accuracy 0.667\n",
      "train loss 1.050 val loss 1.073 and val accuracy 0.668\n",
      "train loss 1.048 val loss 1.071 and val accuracy 0.668\n",
      "train loss 1.045 val loss 1.069 and val accuracy 0.669\n",
      "train loss 1.043 val loss 1.067 and val accuracy 0.670\n",
      "train loss 1.040 val loss 1.064 and val accuracy 0.670\n",
      "train loss 1.038 val loss 1.062 and val accuracy 0.671\n",
      "train loss 1.036 val loss 1.060 and val accuracy 0.670\n",
      "train loss 1.034 val loss 1.058 and val accuracy 0.670\n",
      "train loss 1.032 val loss 1.056 and val accuracy 0.670\n",
      "train loss 1.030 val loss 1.054 and val accuracy 0.672\n",
      "train loss 1.028 val loss 1.052 and val accuracy 0.672\n",
      "train loss 1.026 val loss 1.050 and val accuracy 0.672\n",
      "train loss 1.024 val loss 1.049 and val accuracy 0.672\n",
      "train loss 1.022 val loss 1.047 and val accuracy 0.672\n",
      "train loss 1.021 val loss 1.046 and val accuracy 0.672\n",
      "train loss 1.019 val loss 1.044 and val accuracy 0.673\n",
      "train loss 1.018 val loss 1.043 and val accuracy 0.673\n",
      "train loss 1.016 val loss 1.042 and val accuracy 0.673\n",
      "train loss 1.015 val loss 1.041 and val accuracy 0.673\n",
      "train loss 1.013 val loss 1.039 and val accuracy 0.674\n",
      "train loss 1.012 val loss 1.038 and val accuracy 0.674\n",
      "train loss 1.011 val loss 1.037 and val accuracy 0.674\n",
      "train loss 1.009 val loss 1.036 and val accuracy 0.673\n",
      "train loss 1.008 val loss 1.035 and val accuracy 0.674\n",
      "train loss 1.007 val loss 1.034 and val accuracy 0.674\n",
      "train loss 1.006 val loss 1.033 and val accuracy 0.674\n",
      "train loss 1.005 val loss 1.032 and val accuracy 0.674\n",
      "train loss 1.004 val loss 1.031 and val accuracy 0.675\n",
      "train loss 1.002 val loss 1.030 and val accuracy 0.675\n",
      "train loss 1.001 val loss 1.029 and val accuracy 0.676\n",
      "train loss 1.000 val loss 1.028 and val accuracy 0.676\n",
      "train loss 0.999 val loss 1.027 and val accuracy 0.677\n",
      "train loss 0.998 val loss 1.026 and val accuracy 0.677\n",
      "train loss 0.997 val loss 1.025 and val accuracy 0.676\n",
      "train loss 0.996 val loss 1.024 and val accuracy 0.677\n",
      "train loss 0.996 val loss 1.024 and val accuracy 0.676\n",
      "train loss 0.995 val loss 1.023 and val accuracy 0.676\n",
      "train loss 0.994 val loss 1.022 and val accuracy 0.677\n",
      "train loss 0.993 val loss 1.021 and val accuracy 0.676\n",
      "train loss 0.992 val loss 1.021 and val accuracy 0.676\n",
      "train loss 0.991 val loss 1.020 and val accuracy 0.676\n",
      "train loss 0.991 val loss 1.019 and val accuracy 0.677\n",
      "train loss 0.990 val loss 1.018 and val accuracy 0.677\n",
      "train loss 0.989 val loss 1.018 and val accuracy 0.677\n",
      "train loss 0.988 val loss 1.017 and val accuracy 0.677\n",
      "train loss 0.988 val loss 1.016 and val accuracy 0.677\n",
      "train loss 0.987 val loss 1.016 and val accuracy 0.678\n",
      "train loss 0.986 val loss 1.015 and val accuracy 0.677\n",
      "train loss 0.986 val loss 1.015 and val accuracy 0.678\n",
      "train loss 0.985 val loss 1.014 and val accuracy 0.678\n",
      "train loss 0.984 val loss 1.013 and val accuracy 0.678\n",
      "train loss 0.984 val loss 1.013 and val accuracy 0.678\n",
      "train loss 0.983 val loss 1.012 and val accuracy 0.678\n",
      "train loss 0.982 val loss 1.012 and val accuracy 0.678\n",
      "train loss 0.982 val loss 1.011 and val accuracy 0.680\n",
      "train loss 0.981 val loss 1.011 and val accuracy 0.680\n",
      "train loss 0.981 val loss 1.010 and val accuracy 0.679\n",
      "train loss 0.980 val loss 1.010 and val accuracy 0.680\n",
      "train loss 0.979 val loss 1.009 and val accuracy 0.680\n",
      "train loss 0.979 val loss 1.009 and val accuracy 0.679\n",
      "train loss 0.978 val loss 1.008 and val accuracy 0.679\n",
      "train loss 0.978 val loss 1.008 and val accuracy 0.680\n",
      "train loss 0.977 val loss 1.007 and val accuracy 0.678\n",
      "train loss 0.977 val loss 1.007 and val accuracy 0.678\n",
      "train loss 0.976 val loss 1.007 and val accuracy 0.678\n",
      "train loss 0.976 val loss 1.006 and val accuracy 0.679\n",
      "train loss 0.975 val loss 1.006 and val accuracy 0.679\n",
      "train loss 0.975 val loss 1.005 and val accuracy 0.679\n",
      "train loss 0.974 val loss 1.005 and val accuracy 0.679\n",
      "train loss 0.974 val loss 1.005 and val accuracy 0.680\n",
      "train loss 0.974 val loss 1.004 and val accuracy 0.679\n",
      "train loss 0.973 val loss 1.004 and val accuracy 0.679\n",
      "train loss 0.973 val loss 1.003 and val accuracy 0.681\n",
      "train loss 0.972 val loss 1.003 and val accuracy 0.681\n",
      "train loss 0.972 val loss 1.003 and val accuracy 0.681\n",
      "train loss 0.971 val loss 1.002 and val accuracy 0.681\n",
      "train loss 0.971 val loss 1.002 and val accuracy 0.681\n",
      "train loss 0.971 val loss 1.002 and val accuracy 0.681\n",
      "train loss 0.970 val loss 1.001 and val accuracy 0.681\n",
      "train loss 0.970 val loss 1.001 and val accuracy 0.681\n",
      "train loss 0.969 val loss 1.001 and val accuracy 0.681\n",
      "train loss 0.969 val loss 1.000 and val accuracy 0.681\n",
      "train loss 0.969 val loss 1.000 and val accuracy 0.681\n",
      "train loss 0.968 val loss 1.000 and val accuracy 0.681\n",
      "train loss 0.968 val loss 0.999 and val accuracy 0.681\n",
      "train loss 0.968 val loss 0.999 and val accuracy 0.681\n",
      "train loss 0.967 val loss 0.999 and val accuracy 0.681\n",
      "train loss 0.967 val loss 0.999 and val accuracy 0.681\n",
      "train loss 0.967 val loss 0.998 and val accuracy 0.681\n",
      "train loss 0.966 val loss 0.998 and val accuracy 0.681\n",
      "train loss 0.966 val loss 0.998 and val accuracy 0.681\n",
      "train loss 0.966 val loss 0.997 and val accuracy 0.681\n",
      "train loss 0.965 val loss 0.997 and val accuracy 0.681\n",
      "train loss 0.965 val loss 0.997 and val accuracy 0.681\n",
      "train loss 0.965 val loss 0.997 and val accuracy 0.681\n",
      "train loss 0.964 val loss 0.996 and val accuracy 0.681\n",
      "train loss 0.964 val loss 0.996 and val accuracy 0.681\n",
      "train loss 0.964 val loss 0.996 and val accuracy 0.681\n",
      "train loss 0.964 val loss 0.996 and val accuracy 0.681\n",
      "train loss 0.963 val loss 0.995 and val accuracy 0.681\n",
      "train loss 0.963 val loss 0.995 and val accuracy 0.681\n",
      "train loss 0.963 val loss 0.995 and val accuracy 0.681\n",
      "train loss 0.962 val loss 0.995 and val accuracy 0.681\n",
      "train loss 0.962 val loss 0.995 and val accuracy 0.682\n",
      "train loss 0.962 val loss 0.994 and val accuracy 0.681\n",
      "train loss 0.962 val loss 0.994 and val accuracy 0.683\n",
      "train loss 0.961 val loss 0.994 and val accuracy 0.683\n",
      "train loss 0.961 val loss 0.994 and val accuracy 0.683\n",
      "train loss 0.961 val loss 0.994 and val accuracy 0.683\n",
      "train loss 0.961 val loss 0.993 and val accuracy 0.683\n",
      "train loss 0.960 val loss 0.993 and val accuracy 0.683\n",
      "train loss 0.960 val loss 0.993 and val accuracy 0.683\n",
      "train loss 0.960 val loss 0.993 and val accuracy 0.683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.960 val loss 0.993 and val accuracy 0.683\n",
      "train loss 0.959 val loss 0.992 and val accuracy 0.683\n",
      "train loss 0.959 val loss 0.992 and val accuracy 0.683\n",
      "train loss 0.959 val loss 0.992 and val accuracy 0.683\n",
      "train loss 0.959 val loss 0.992 and val accuracy 0.683\n",
      "train loss 0.959 val loss 0.992 and val accuracy 0.683\n",
      "train loss 0.958 val loss 0.991 and val accuracy 0.683\n",
      "train loss 0.958 val loss 0.991 and val accuracy 0.684\n",
      "train loss 0.958 val loss 0.991 and val accuracy 0.684\n",
      "train loss 0.958 val loss 0.991 and val accuracy 0.684\n",
      "train loss 0.957 val loss 0.991 and val accuracy 0.683\n",
      "train loss 0.957 val loss 0.991 and val accuracy 0.683\n",
      "train loss 0.957 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.957 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.957 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.956 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.956 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.956 val loss 0.990 and val accuracy 0.683\n",
      "train loss 0.956 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.956 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.955 val loss 0.989 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.954 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.988 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.953 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.987 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.952 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.986 and val accuracy 0.683\n",
      "train loss 0.951 val loss 0.985 and val accuracy 0.683\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.683\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.683\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.683\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.950 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.985 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.684\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.949 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.984 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.983 and val accuracy 0.685\n",
      "train loss 0.948 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.948 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.948 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.947 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.946 val loss 0.983 and val accuracy 0.684\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.946 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.982 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.945 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.944 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.981 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.685\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.686\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.686\n",
      "train loss 0.943 val loss 0.980 and val accuracy 0.686\n"
     ]
    }
   ],
   "source": [
    "model = MultiLogisticRegression(55, 18)\n",
    "train_epochs(model, x_train, y_train, x_valid, y_valid, 300, lr=0.05, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
